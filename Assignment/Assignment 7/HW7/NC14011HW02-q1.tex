\subsection*{الف}

$$
y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}, \quad \text{for} \quad j = 1 \text{ to } n
$$

در صورتی که یکی از
$z_i$
ها برابر بی‌نهایت شود، 
\lr{softmax}
به 
$1$
میل می‌کند چون تمام مقادیر غیر از 
$z_i$
به ۰ میل می‌کنند.

\subsection*{ب}

مدل پرسپترون یکی از مدل‌های پایه و مبنایی شبکه‌های عصبی مصنوعی است و به عنوان یک نمایش ساده از نورون‌های بیولوژیکی در نظر گرفته می‌شود. این مدل برای یادگیری نظارت‌شده کلاس‌بندی باینری (تقسیم داده‌ها به دو کلاس) استفاده می‌شود. پرسپترون یک بردار از ویژگی‌های ورودی را دریافت می‌کند و به هر ویژگی وزنی اختصاص می‌دهد. سپس یک جمع وزن‌دار از ورودی‌ها محاسبه می‌شود و تابع فعال‌سازی را بر روی آن اعمال کرده تا خروجی تولید شود.

$$
output = activation \hspace{1mm} function (w_1 \times x_1 + w_2 \times x_2 + ... + w_n \times x_n + bias)
$$

حالا که دو ورودی داریم پس برای خروجی داریم:

$$
output = activation \hspace{1mm} function (w_1 \times x_1 + w_2 \times x_2 + bias)
$$


بله، این کار امکان‌پذیر است. پرسپترون می‌تواند توابع منطقی ساده مثل
\lr{AND}
و
\lr{OR}
را تقلید کند. در زیر، این کار را برای هر دو تابع با وزن‌ها و آستانه بیان می‌کنیم.

\lr{AND Perceptron}

بله، این کار امکان‌پذیر است. پرسپترون می‌تواند توابع منطقی ساده مثل
\lr{AND}
و
\lr{OR}
را تقلید کند. در زیر، این کار را برای هر دو تابع با وزن‌ها và آستانه بیان می‌کنیم.
\lr{AND Perceptron}
برای یک پرسپترون که تابع منطقی VÀ را تقلید کند، ما می‌توانیم وزن‌ها و آستانه (بایاس) را به شکل زیر تعیین کنیم:
$$
w_1 = 1, \quad w_2 = 1, \quad \text{bias} = -1.5
$$

در این صورت، برای ورودی‌های 
$(0, 0)$، $(0, 1)$، $(1, 0)$ 
خروجی 
$0$ 
و برای ورودی 
$(1, 1)$ 
خروجی 
$1$ 
خواهیم داشت، که دقیقا عملکرد گیت
\lr{AND}
است.

\lr{OR Perceptron}

برای یک پرسپترون که تابع منطقی
\lr{OR}
را تقلید کند، ما می‌توانیم وزن‌ها و آستانه را به شکل زیر تعیین کنیم:

$$
w_1 = 1, \quad w_2 = 1, \quad \text{bias} = -0.5
$$

در این صورت، برای ورودی $(0, 0)$ خروجی $0$ و برای ورودی‌های $(0, 1)$، $(1, 0)$، $(1, 1)$ خروجی $1$ خواهیم داشت، که دقیقا عملکرد گیت
\lr{OR}
است.

در عمل، مدل پرسپترون توسط الگوریتمی مثل 
\lr{gradient descent}
آموزش داده می‌شود تا وزن‌ها و بایاس را پیدا کند. ولی در این مثال ساده، ما می‌توانیم به صورت دستی آن‌ها را مشخص کنیم.

\lr{XOR Perceptron}


یک پرسپترون تنها نمی‌تواند تابع
\lr{XOR}
را مدل کند. دلیل این است که
\lr{XOR}
یک تابع که غیرخطی است و پرسپترون‌ها فقط قادر به مدل‌سازی توابع خطی هستند. در واقع، این یکی از محدودیت‌های اصلی پرسپترون‌ها است که منجر به توسعه شبکه‌های عصبی چندلایه شده است.

با این حال، می‌توانیم یک شبکه‌ی پرسپترون چند لایه
\lr{(MLP)}
با استفاده از دو گیت
\lr{AND}
و
\lr{OR}
و یک گیت
\lr{NOT}
بسازیم که تابع
\lr{XOR}
را مدل کند.

مدل
\lr{MLP}
به این صورت خواهد بود:

لایه‌ی اول (پنهان): دو نورون، یکی برای گیت
\lr{AND}
و دیگری برای گیت
\lr{OR}
:

$$
AND \hspace{1.5mm} Perceptron: w_1 = 1, w_2 = 1, bias = -1.5
$$

$$
OR \hspace{1.5mm} Perceptron: w_1 = 1, w_2 = 1, bias = -0.5
$$

لایه‌ی دوم (خروجی): یک نورون با گیت
\lr{NOT}
برای خروجی
\lr{AND}
و
\lr{OR}
از لایه‌ی پنهان:
$$
NOT \hspace{1.5mm} Perceptron: w_1 = -2, w_2 = 1, bias = 0.5
$$

در اینجا، خروجی گیت
\lr{AND}
در ورودی گیت
\lr{NOT}
قرار می‌گیرد. به این ترتیب، گیت
\lr{NOT}
خروجی گیت
\lr{AND}
را انکار می‌کند، در حالی که گیت
\lr{OR}
بدون تغییر باقی می‌ماند. این منجر به تابع
\lr{XOR}
می‌شود.

