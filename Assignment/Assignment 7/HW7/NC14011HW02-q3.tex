\subsection*{الف}

$$
\frac{\partial cost}{\partial w_0} = 2 (y - h \Theta (x)) (x - 1) \times  \frac{\partial h}{\partial z} \times \frac{\partial z}{\partial w_0} = -2 (y - h) h (1 - h)
$$

$$
\frac{\partial cost}{\partial w_1} = -2 x_1 (y - h) (1 - h) h
$$

$$
\frac{\partial cost}{\partial w_2} = -2 x_2 (y - h) (1 - h) h
$$

تابع هزینه و گرادیان های مربوطه را در نظر بگیرید، برای بروزرسانی وزن ها در هر دور تکرار از گرادیان نزولی، ما می توانیم از قاعده زیر استفاده کنیم:

$$
w_{i,\text{{new}}} = w_{i,\text{{old}}} - \alpha \frac{\partial \text{{cost}}}{\partial w_i}
$$

که در آن 
$\alpha$
نرخ یادگیری است.

اگر ما 
$\alpha$
را برابر با 1 در نظر بگیریم، ما داریم:

\begin{align}
	w_{0,\text{{new}}} &= w_{0,\text{{old}}} + 2 (y - h) h (1 - h) \\
	w_{1,\text{{new}}} &= w_{1,\text{{old}}} + 2 x_1 (y - h) (1 - h) h \\
	w_{2,\text{{new}}} &= w_{2,\text{{old}}} + 2 x_2 (y - h) (1 - h) h
\end{align}

به طور کلی، نرخ یادگیری 
$\alpha$
در فرمول‌های بروزرسانی را تنظیم می کنیم تا کنترل کنیم که گام‌های بروزرسانی چقدر بزرگ باشند. اگر 
$\alpha$
خیلی بزرگ باشد، ما ممکن است از حداقل محلی عبور کنیم و اگر خیلی کوچک باشد، فرآیند آموزش ممکن است خیلی کند باشد.

\subsection*{ب}

ما معمولا تابع سیگموید را به عنوان تابع فعال‌سازی در لجستیک رگرسیون یا شبکه های عصبی استفاده می‌کنیم، اما از آن به عنوان تابع هزینه استفاده نمی‌کنیم. دلیل اصلی این است که هنگامی که تابع سیگموید را به عنوان تابع هزینه استفاده می‌کنیم، ممکن است با مشکل «هزینه محلی مینیمم» 
\lr{(local minimal)}
مواجه شویم.

به طور خاص، گرادیان های تابع سیگموید ممکن است خیلی کوچک شوند، یک مشکل به نام 
\lr{vanishing gradients}
که می‌تواند منجر به توقف یا تاخیر زیادی در یادگیری شود. در حقیقت، تابع سیگموید می‌تواند برای مقادیر خیلی بزرگ یا کوچک، گرادیان های خیلی کوچکی داشته باشد که می‌تواند فرآیند یادگیری را کند کند.

به همین دلیل، ما معمولا از تابع هزینه 
\lr{cross-entropy}
در لجستیک رگرسیون استفاده می‌کنیم، که توانایی بهتری در مدیریت این مشکلات دارد. تابع هزینه
\lr{cross-entropy}
به خوبی با تابع فعال‌سازی سیگموید ترکیب می‌شود و منجر به گرادیان های بهتری در فرآیند یادگیری می‌شود.

در این مثال و بخش ب، تابع دوم در لوکال مینیمم‌ها زیاد گیر می‌کند اگر از
\lr{gradient descent}
استفاده کنیم چون یک تابع محدب نیست ولی اولی این مشکل را ندارد.