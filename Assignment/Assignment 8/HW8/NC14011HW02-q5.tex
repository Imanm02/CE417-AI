\subsection*{الف}

یکی از استفاده‌های اصلی کانولوشن
\lr{1x1}
در شبکه های عصبی پیچشی یا
\lr{CNN}
، تغییر تعداد کانال‌ها 
\lr{(depth)}
است. این موضوع در ارتباط با مدل‌های معماری شبکه‌های عمیق مانند
\lr{GoogLeNet}
که معماری
\lr{Inception}
را ارائه کرد، به‌ویژه مهم است. در این معماری، یک سری کانولوشن‌های
\lr{1x1}
(که اغلب به عنوان «باتل‌نک» شناخته می‌شوند) مورد استفاده قرار گرفتند تا تعداد کانال‌های ورودی کاهش یابد قبل از اعمال کانولوشن‌های
\lr{3x3}
یا
\lr{5x5}

این کاهش در تعداد کانال‌ها باعث کاهش مصرف حافظه و افزایش سرعت آموزش می‌شود.

در مدل‌های معماری دیگر مانند
\lr{ResNet}
نیز، کانولوشن‌های
\lr{1x1}
برای تغییر تعداد کانال‌ها (و گاهی اوقات تغییر اندازه خروجی مکانی) استفاده می‌شوند.

به طور کلی، کانولوشن
\lr{1x1}
در
\lr{CNN}
می‌تواند به چندین شکل مفید باشد:

کاهش تعداد کانال‌ها: مانند چیزی که در بالا توضیح داده شد.
افزایش تعداد کانال‌ها: اگر بخواهیم تعداد کانال‌ها را افزایش دهیم، کانولوشن
\lr{1x1}
می‌تواند یک راه مناسب باشد.

استفاده به عنوان یک تابع غیرخطی: با استفاده از تابع فعال‌سازی غیرخطی بعد از کانولوشن
\lr{1x1}
، می‌توانیم مدل را پیچیده‌تر کنیم.

\subsection*{ب}

وقتی شما می خواهید سایز ورودی و خروجی لایه کانولوشن برابر باشد، شما باید از
\lr{"same padding"}
استفاده کنید. در این حالت، ورودی به گونه‌ای 
\lr{padding}
 می شود تا ابعاد ورودی و خروجی برابر باشند.

مقدار 
\lr{padding}
 وابسته به سایز کرنل کانولوشن است. اگر سایز کرنل
\lr{K}
باشد و از گام 1 
\lr{(Stride 1)}
استفاده کنیم، آنگاه مقدار 
\lr{padding}
 باید
$\frac{k-1}{2}$
باشد. به عنوان مثال، اگر از یک کرنل
\lr{3x3}
استفاده کنید، مقدار 
\lr{padding}
 باید 1 باشد تا ابعاد ورودی و خروجی برابر باشند. اگر از یک کرنل
\lr{5x5}
استفاده کنید، 
\lr{padding}
 باید 2 باشد.

لطفا توجه داشته باشید که این قاعده برای گام 1 صادق است. اگر از گام بزرگتر استفاده کنید، ممکن است نتوانید سایز ورودی و خروجی را برابر کنید.

\subsection*{ج}

\lr{average pooling}
و
\lr{max pooling}
چه تفاوت‌هایی دارند و استفاده از هر کدام در چه مواردی بهتر است؟

جواب:
در جواب باید گفت که
\lr{"Max Pooling"}
و 
\lr{"Average Pooling"}
دو روش کاهش سایزیتی در شبکه‌های عصبی پیچشی
\lr{(CNN)}
هستند. این دو روش با گرفتن خروجی‌های کانولوشن از قبلی و کاهش ابعاد آنها به وسیله کاهش سایزتی، به استخراج ویژگی‌ها کمک می‌کنند. اما تفاوت‌های اصلی آن‌ها در نحوه عملکردشان و کاربرد آن‌هاست.

\lr{Max Pooling}
: این روش در هر پنجره، بزرگترین عدد (یعنی ویژگی با بیشترین حضور) را انتخاب می‌کند. به این ترتیب،
\lr{Max Pooling}
می‌تواند ویژگی‌های برجسته را در نقشه‌های ویژگی حفظ کند، حتی اگر این ویژگی‌ها کوچک باشند. این ویژگی اغلب در شناسایی اشیا و تصاویر مفید است، زیرا ویژگی‌های برجسته مهمتر از میانگین ویژگی‌های دیگر هستند.

\lr{Average Pooling}
: این روش میانگین تمام اعداد در هر پنجره را محاسبه می‌کند. این باعث می‌شود که نقشه‌های ویژگی بیشتری از ویژگی‌های عمومی تصویر را نگه دارند و می‌تواند کمک کند تا شبکه در برابر تغییرات مکانی مقاومتر شود. این روش اغلب در مواردی استفاده می‌شود که نیاز به دید کلی از ویژگی‌های تصویر داریم، نه فقط ویژگی‌های برجسته.

این‌که کدام یک بهتر است بستگی به کاربرد خاص شما دارد. به طور کلی،
\lr{Max Pooling}
در بسیاری از شبکه‌های
\lr{CNN}
که برای تشخیص تصویر استفاده می‌شوند، محبوب‌تر است زیرا می‌تواند ویژگی‌های برجسته را حفظ کند. با این حال،
\lr{Average Pooling}
می‌تواند در بعضی از موارد، مانند شناسایی حضور یا عدم حضور یک ویژگی کلی در تصویر، مفید باشد.