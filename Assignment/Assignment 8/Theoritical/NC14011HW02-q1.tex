\subsection*{الف}

مقداردهی اولیه همه پارامترها با صفر در مدل‌های خطی ساده مانند رگرسیون لجستیک، گاهی اوقات می‌تواند باعث مشکلاتی شود. در مورد رگرسیون لجستیک، اگر همه وزن‌ها صفر باشند، مدل در شروع آموزش برای تمام نمونه‌ها پیش‌بینی مشابهی انجام خواهد داد چون خروجی تابع سیگموید (که در رگرسیون لجستیک استفاده می‌شود) در صفر،
\lr{0.5}
است.

بنابراین، در شروع آموزش، تمام نمونه‌ها با احتمال
\lr{0.5}
 دسته‌بندی می‌شوند. این می‌تواند سرعت یادگیری مدل را کاهش دهد، چون گرادیان‌ها برای تمامی ویژگی‌ها یکسان خواهند بود و همه وزن‌ها با یک سرعت یاد می‌گیرند.

به همین دلیل، معمولا ترجیح می‌دهیم که پارامتر‌ها را با اعداد تصادفی کوچک مقداردهی اولیه کنیم. این روش به ما امکان می‌دهد که تنوع بیشتری در آغاز یادگیری داشته باشیم و از گیر کردن در حداقل‌های محلی جلوگیری کنیم.

همچنین لازم به ذکر است که در مدل‌های عمیق‌تر مانند شبکه‌های عصبی، مقداردهی اولیه همه پارامتر‌ها با صفر می‌تواند منجر به مشکلات جدی‌تری شود، مانند «مشکل گرادیان‌های ناپدید شونده» یا «مشکل نرون‌های مرده».

مشکل نرون‌های مرده: در شبکه‌های عصبی با توابع فعال‌سازی مانند
\lr{ReLU (Rectified Linear Unit)}
، اگر یک نرون در شروع آموزش یک ورودی منفی دریافت کند، آنگاه خروجی آن
\lr{0}
 خواهد بود (چون
\lr{ReLU}
 برای ورودی‌های منفی صفر است). اگر این نرون همچنان ورودی‌های منفی دریافت کند، گرادیان آن نیز صفر خواهد بود و وزن‌های آن در فرآیند بروزرسانی با گرادیان کاهش یافته تغییر نمی‌کند. این نرون «مرده» می‌شود و دیگر هیچ تاثیری در شبکه ندارد. اگر همه وزن‌ها با صفر مقداردهی اولیه شوند، همه نرون‌ها در ابتدا «مرده» هستند و یادگیری اتفاق نمی‌افتد.

مشکل گرادیان‌های ناپدید شونده: اگر همه وزن‌ها با یک مقدار ثابت (مانند صفر) مقداردهی اولیه شوند، در هر لایه، همه نرون‌ها یک گرادیان یکسان دریافت می‌کنند و بروزرسانی مشابهی را انجام می‌دهند. این باعث می‌شود که همه نرون‌ها همان فرایند یادگیری را تجربه کنند، که باعث محدود کردن قدرت شبکه می‌شود. همچنین، در شبکه‌های عمیق، این می‌تواند منجر به مشکل گرادیان‌های ناپدید شونده شود، که در آن گرادیان‌ها در هر لایه به طور متوالی کاهش یافته و به لایه‌های اولیه شبکه نمی‌رسند.

به همین دلیل، ما معمولا از مقداردهی اولیه تصادفی برای وزن‌ها استفاده می‌کنیم. این کمک می‌کند تا نرون‌ها در ابتدای یادگیری تفاوت‌هایی داشته باشند و شبکه بتواند از تمام نرون‌ها به طور موثر استفاده کند. برخی از روش‌های مقداردهی اولیه معروف شامل
\lr{Xavier/Glorot Initialization}
و
\lr{He Initialization}
هستند که بر اساس توزیع‌های خاصی از اعداد تصادفی، وزن‌ها را مقداردهی اولیه می‌کنند.

\subsection*{ب}

اگر از
\lr{Full Batch Gradient Descent}
استفاده می‌کنید، شافل کردن داده‌ها لزوماً تأثیر زیادی بر عملکرد یادگیری نخواهد داشت. دلیل این است که در
\lr{Full Batch Gradient Descent}
، گرادیان‌ها بر اساس کل داده‌ها محاسبه می‌شوند، بنابراین ترتیب داده‌ها تأثیری بر گرادیان محاسبه شده نخواهد داشت.

با این حال، در متد‌های
\lr{Mini-Batch Gradient Descent}
یا
\lr{Stochastic Gradient Descent}
، که در هر بروزرسانی فقط از یک زیرمجموعه از داده‌ها یا یک داده استفاده می‌کنند، شافل کردن داده‌ها می‌تواند مهم باشد. این کار می‌تواند به اطمینان برساند که مدل ما از تمامی داده‌ها به طور مناسب یاد بگیرد و از یادگیری ترتیب خاصی از داده‌ها جلوگیری کند. این می‌تواند به جلوگیری از بیش‌برازش کمک کند و عمومیت مدل را افزایش دهد.

شافل کردن داده‌ها در یادگیری
\lr{Mini-Batch}
و
\lr{Stochastic}
نه تنها از مدل جلوگیری می‌کند که یک الگوی خاص در ترتیب داده‌ها را یاد بگیرد، بلکه باعث می‌شود تا هر بروزرسانی وزن‌ها بر پایه‌ی یک نمونه تصادفی از داده‌ها انجام شود. این می‌تواند کمک کند تا مدل از گیر کردن در حداقل‌های محلی جلوگیری کند و به‌دنبال یافتن یک حداقل جهانی باشد، زیرا هر بروزرسانی تصادفی اجازه می‌دهد تا گرادیان به سمت‌های مختلفی حرکت کند.

این فرایند، همچنین به توزیع یکنواخت بیشتری از داده‌ها در طول آموزش منجر می‌شود، که می‌تواند به بهبود عمومیت مدل کمک کند.

به هر حال، استفاده از شافل داده‌ها یک تکنیک مفید است که معمولاً به عنوان یک قسمت از پیش‌پردازش داده‌ها قبل از آموزش مدل اعمال می‌شود. هرچند در
\lr{Full Batch Gradient Descent}
لزومی به آن نیست، اما در آموزش‌هایی که بروزرسانی‌ها بر اساس زیرمجموعه‌هایی از داده‌ها انجام می‌شوند، می‌تواند بسیار مفید باشد.

\subsection*{ج}

بله، شافل کردن داده‌ها در این مورد بسیار مهم است.

وقتی از روش
\lr{Mini-Batch Gradient Descent}
استفاده می‌کنیم، ما داده‌ها را به چندین بسته
\lr{(batch)}
تقسیم می‌کنیم و سپس گرادیان را برای هر بسته محاسبه می‌کنیم. اگر تمامی تصاویر سگ‌ها در ابتدای داده‌ها قرار داشته باشند و تمام تصاویر گربه‌ها در انتها، در طول یک دوره آموزش
\lr{(epoch)}
، مدل ابتدا فقط بر روی تصاویر سگ‌ها آموزش می‌بیند و سپس فقط بر روی تصاویر گربه‌ها. این باعث می‌شود مدل در قسمت‌های مختلف دوره آموزش، یک الگوی خاص را یاد بگیرد و ممکن است باعث بیش‌برازش
\lr{(overfitting)}
شود.

وقتی داده‌ها را شافل می‌کنیم، ما از این اطمینان حاصل می‌شویم که در طول هر دوره آموزش، مدل بر روی ترکیبی تصادفی از تصاویر سگ‌ها و گربه‌ها آموزش می‌بیند، به جای یک دسته خاص. این به مدل کمک می‌کند تا تعمیم بیشتری بر روی داده‌ها پیدا کند و باعث کاهش بیش‌برازش می‌شود.

به علاوه، در صورت عدم شافل کردن، مدل ممکن است دچار تغییرات ناگهانی در گرادیان شود زیرا بسته‌های ابتدایی شامل تصاویر متفاوتی با بسته‌های انتهایی هستند. این ممکن است موجب ناپایداری در فرآیند یادگیری شود. بنابراین، شافل کردن داده‌ها می‌تواند به استقرار بیشتر در فرآیند یادگیری کمک کند.

در ادامه، شافل کردن داده‌ها قبل از آموزش می‌تواند به مدل کمک کند تا موثرتر یاد بگیرد. این امر به دو دلیل است:

پیشگیری از ترتیب خاص داده‌ها: با شافل کردن داده‌ها، ما می‌توانیم از یادگیری یک ترتیب خاص داده‌ها توسط مدل جلوگیری کنیم. اگر ترتیب خاصی در داده‌ها وجود داشته باشد و مدل بتواند آن را یاد بگیرد، این ممکن است باعث کاهش توانایی عمومی‌سازی مدل بر روی داده‌های جدید شود.

یادگیری متنوع‌تر: با شافل کردن داده‌ها، ما مطمئن می‌شویم که در طول هر دوره آموزش، مدل بر روی یک ترکیب متنوع از نمونه‌ها آموزش می‌بیند. این به مدل اجازه می‌دهد تا در طول زمان، با یک توزیع بیشتری از داده‌ها مواجه شود، که می‌تواند به بهبود عمومیت مدل کمک کند.

با توجه به این دلایل، شافل کردن داده‌ها قبل از آموزش یک مرحله مهم و پیشنهادی در پردازش داده‌ها است. حتی اگر برخی از موارد (مانند
\lr{Full Batch Gradient Descent}
) ممکن است از این تکنیک به طور مستقیم سود نبرند، شافل کردن داده‌ها همچنان می‌تواند برای ارائه یک توزیع متنوع‌تر از داده‌ها و افزایش توانایی عمومی‌سازی مدل مفید باشد.

\subsection*{د}

\lr{Batch Gradient Descent (BGD)}
:

نمودار آموزش در این روش به صورت منظم و یکنواخت کاهش می‌یابد. چراکه در
BGD
، ما گرادیان را بر اساس کل داده‌های آموزش محاسبه می‌کنیم، بنابراین در هر بروزرسانی، ما به سمتی حرکت می‌کنیم که بر اساس تمام داده‌های آموزش تعیین شده است. این باعث می‌شود تا نمودار آموزش BGD خطی و یکنواخت باشد.

\lr{Mini-Batch Gradient Descent (MBGD)}
:

نمودار آموزش این روش می‌تواند کمی ناپایدارتر باشد. زیرا در
\lr{MBGD}
، ما فقط بر اساس یک زیرمجموعه از داده‌های آموزش گرادیان را محاسبه می‌کنیم. این می‌تواند باعث شود تا نمودار آموزش
\lr{MBGD}
کمی نوسان داشته باشد، اما به طور کلی باید همچنان به سمت کاهش کلی خطا حرکت کند.

\lr{Stochastic Gradient Descent (SGD)}
:

نمودار آموزش این روش احتمالاً ناپایدارترین است. زیرا در
\lr{SGD}
، ما گرادیان را بر اساس یک نمونه تصادفی از داده‌های آموزش محاسبه می‌کنیم. این باعث می‌شود تا نمودار آموزش
\lr{SGD}
نوسانات زیادی داشته باشد. با این حال، اگر مقدار یادگیری به درستی تنظیم شود،
\lr{SGD}
همچنان می‌تواند به سمت کاهش کلی خطا حرکت کند، اما ممکن است به یک حالت مینیمم محلی برسد به جای مینیمم جهانی.

در نهایت، باید توجه داشت که این توضیحات فرض می‌کنند که تمامی این روش‌ها با سرعت یادگیری مناسب تنظیم شده‌اند. اگر سرعت یادگیری خیلی بزرگ یا خیلی کوچک باشد، ممکن است باعث شود نمودارها رفتار غیرمنتظره‌ای داشته باشند.

با توجه به این توضیحات،
نمودار ۱ برای
\lr{Batch Gradient Descent (BGD)}
و نمودار ۲ برای
\lr{Mini-Batch Gradient Descent (MBGD)}
و نمودار ۳ برای 
\lr{Stochastic Gradient Descent (SGD)}
می‌باشد با توجه به میزان نوسان آن‌ها.
