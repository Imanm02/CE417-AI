\subsection*{الف}

روش
\lr{Value Iteration}
یکی از الگوریتم‌های حل مسئله در یادگیری تقویتی است که برای حل مسئله‌های مارکوف تصمیم‌گیری
\lr{(MDP)}
استفاده می‌شود. این الگوریتم به صورت تکراری و تمامیتی عمل می‌کند و به طور تدریجی مقادیر ارزش برای هر حالت را به‌روزرسانی می‌کند تا به یک تابع ارزش بهینه برسد.

برای استفاده از الگوریتم
\lr{Value Iteration}
، نیازمند تعریف
\lr{MDP}
هستیم.
\lr{MDP}
توسط چهار مؤلفه اصلی تعریف می‌شود:

مجموعه حالات
\lr{(States)}:
مجموعه‌ای از حالات قابل مشاهده در مسئله را نشان می‌دهد.

مجموعه عمل‌ها 
\lr{(Actions)}:
مجموعه‌ای از عمل‌های قابل انجام در هر حالت را نشان می‌دهد.

تابع انتقال
\lr{(Transition Function)}:
تابعی است که نشان می‌دهد با انجام یک عمل در یک حالت، ما به چه حالتی خواهیم رفت و با چه احتمالی.

تابع پاداش
\lr{(Reward Function)}:
تابعی است که برای هر حالت و عمل، پاداش مشخص می‌کند.

هدف
\lr{Value Iteration}
یافتن تابع ارزش بهینه است که برای هر حالت، ارزش بهینه انتظاری را نشان می‌دهد. الگوریتم
\lr{Value Iteration}
به صورت تکراری عمل می‌کند و به تدریج مقادیر ارزش را به‌روزرسانی می‌کند تا به تابع ارزش بهینه برسد.

حالا که تمام حالات ۰ هستند، تا عمق ۲ را محاسبه می‌کنیم. (همچنین از نوشتن یک سری محاسبات عددی چشم‌پوشی می‌کنیم چون محاسبات بسیار زیادی دارد این سوال.)

\[ V(s) = \max \left[ \sum p(s'|s,a) \cdot \left( R(s,a,s') + \gamma \cdot V(s') \right) \right] \]

پس داریم:

$$
V_1(S_1) = max[1 \times [-1 + V_0 (S_2)] , 1 \times [-1 + V_0 (S_6)] , 0.6 \times (3 + V_{goal}) + 0.4 \times (-10 + V_{fail})]
$$

$$
\rightarrow V_1(S_1) = max[-1 , -1 , -2.2] = -1
$$

به همین شکل برای باقی نیز خواهیم داشت:

$$
\rightarrow V_1(S_2) = max[-1 , -1 , -1 , -1] = -1
$$

$$
\rightarrow V_1(S_3) = max[-1 , -1 , -1 , 0.8] = 0.8
$$

$$
\rightarrow V_1(S_4) = max[-1 , -1 , -1 , -1] = -1
$$

$$
\rightarrow V_1(S_5) = max[-1 , -1 , -2.2] = -1
$$

$$
\rightarrow V_1(S_6) = max[-1 , -1 , -2.2] = -1
$$

$$
\rightarrow V_1(S_7) = max[-1 , -1 , -1 , -1] = -1
$$

$$
\rightarrow V_1(S_8) = max[-1 , -1 , -1 , -1] = -1
$$

$$
\rightarrow V_1(S_9) = max[-1 , -1 , -1 , -1] = -1
$$

$$
\rightarrow V_1(S_{10}) = max[-1 , -1 , -2.2] = -1
$$

حالا یک عمق دیگر پیش می‌رویم:

$$
\rightarrow V_2(S_1) = max[-2 , -2 , -2.2] = -2
$$

$$
\rightarrow V_2(S_2) = max[-2 , -2 , -0.2, -1] = -0.2
$$

$$
\rightarrow V_2(S_3) = max[-2 , -2 , -0.8, -2] = -0.8
$$

$$
\rightarrow V_2(S_4) = max[-2 , -2 , -1, -0.2] = -0.2
$$

$$
\rightarrow V_2(S_5) = max[-2 , -2 , -2.2] = -2
$$

$$
\rightarrow V_2(S_6) = max[-2 , -2 , -2.2] = -2
$$

$$
\rightarrow V_2(S_7) = max[-2 , -2 , -0.2, -1] = -0.2
$$

$$
\rightarrow V_2(S_8) = max[-2 , -2 , -0.2, -1] = -0.2
$$

$$
\rightarrow V_2(S_9) = max[-2 , -2 , -2.2] = -2
$$

$$
\rightarrow V_2(S_{10}) = max[-2 , -2 , -0.2, -1] = -0.2
$$

\subsection*{ب}

الگوریتم
\lr{Q-Learning}
یکی از الگوریتم‌های یادگیری تقویتی است که برای حل مسئله یادگیری تقویتی بدون مدل
\lr{(Model-Free)}
استفاده می‌شود. این الگوریتم به صورت تکراری و تمامیتی عمل می‌کند و به تدریج تابع 
\lr{Q-Value}
را به‌روزرسانی می‌کند تا به تابع
\lr{Q-Value}
بهینه برسد.

برای استفاده از الگوریتم
\lr{Q-Learning}
، نیازمند تعریف یک
\lr{MDP}
هستیم.
\lr{MDP}
توسط چهار مؤلفه اصلی تعریف می‌شود:

مجموعه حالات
\lr{(States)}:
مجموعه‌ای از حالات قابل مشاهده در مسئله را نشان می‌دهد.

مجموعه عمل‌ها 
\lr{(Actions)}:
مجموعه‌ای از عمل‌های قابل انجام در هر حالت را نشان می‌دهد.

تابع انتقال 
\lr{(Transition Function)}:
تابعی است که نشان می‌دهد با انجام یک عمل در یک حالت، ما به چه حالتی خواهیم رفت و با چه احتمالی.

تابع پاداش 
\lr{(Reward Function)}:
تابعی است که برای هر حالت و عمل، پاداش مشخص می‌کند.

هدف
\lr{Q-Learning}
یافتن تابع
\lr{Q-Value}
بهینه است که برای هر حالت و عمل، ارزش بهینه انتظاری را نشان می‌دهد. الگوریتم
\lr{Q-Learning}
به صورت تکراری و مبتنی بر خبره
\lr{(Experience-Based)}
عمل می‌کند و به تدریج مقادیر
\lr{Q-Value}
را به‌روزرسانی می‌کند تا به تابع
\lr{Q-Value}
بهینه برسد.

حالا الگوریتم را ران می‌کنیم:


$$
Episode 1:
$$

$$
Q(s_8 , R) = 0.5 \times -1 = -0.5
$$

$$
Q(s_9 , U) = 0.5 \times -1 = -0.5
$$

$$
Q(s_4 , S) = 0.5 \times 1 = 0.5
$$

$$
Episode 2:
$$

$$
Q(s_8 , S) = 0.5 \times 1 = 0.5
$$

$$
Q(s_7 , R) = 0.5 \times 1 = 0.5
$$

$$
Q(s_8 , R) = 0.5 \times 0.5 + 0.5 = 0.75
$$

$$
Episode 3:
$$

$$
Q(s_{10} , S) = 0.5 \times 2 = 1
$$

$$
Q(s_9 , R) = 0.5 \times 1 = 0.5
$$

$$
Q(s_8 , R) = 0.75 + 0.5 \times 0.25 = 0.875
$$

$$
Episode 4:
$$

$$
Q(s_8 , S) = 0.5 \times 4.5 = 2.25
$$

$$
Q(s_7 , L) = 0.5 \times 0.5 = 0.25
$$

$$
Q(s_8 , R) = 0.875 + 0.5 \times 0.125 = 0.9375
$$

$$
Episode 5:
$$

$$
Q(s_{10} , S) = 1 + 0.5 \times 2 = 2
$$

$$
Q(s_9 , U) = 0.5 + 0.5 \times -0.5 = 0.25
$$

$$
Q(s_8 , R) = 0.9375 + 0.5 \times 0.0625 = 0.96875
$$