\subsection*{الف}

صحیح

\subsection*{ب}

نه، این جمله درست نیست. در عمل، سیاست‌های پیداشده توسط الگوریتم‌های
\lr{Policy Iteration}
و
\lr{Value Iteration}
ممکن است با هم تفاوت داشته باشند و بسته به مسئله و شرایط مختلف، نتایج ممکن است متفاوت باشند. اما در حالت کلی، نمی‌توان گفت که سیاست‌های پیداشده توسط
\lr{Policy Iteration}
ضعیف‌تر از سیاست‌های پیداشده توسط
\lr{Value Iteration}
هستند یا برعکس.

در
\lr{Policy Iteration}
، به‌روزرسانی‌ها در دو مرحله انجام می‌شود: در مرحله اول، تابع ارزش بهینه برای سیاست فعلی به‌روزرسانی می‌شود و در مرحله دوم، سیاست جدید بر اساس تابع ارزش بهینه جدید تعیین می‌شود. به این ترتیب، الگوریتم
\lr{Policy Iteration}
بهبودی پیوسته در سیاست و تابع ارزش بهینه ایجاد می‌کند.

در
\lr{Value Iteration}
، تابع ارزش بهینه به صورت تکراری به‌روزرسانی می‌شود و در هر مرحله، تابع ارزش بهینه به تدریج به مقدار نهایی همگرا می‌شود. در نهایت، می‌توان از تابع ارزش بهینه برای تولید سیاست بهینه استفاده کرد.

به طور کلی، می‌توان گفت که
\lr{Policy Iteration}
و
\lr{Value Iteration}
به دست آوردن سیاست بهینه را هدف دارند و در نهایت به نتایج مشابهی می‌رسند. اما در مسائلی که سیاست‌ها بیش از یک سیاست بهینه دارند، ممکن است سیاست‌های پیداشده توسط
\lr{Policy Iteration}
و
\lr{Value Iteration}
با هم تفاوت داشته باشند. در این حالت، می‌توان گفت که هیچ‌کدام از این الگوریتم‌ها برتری مطلق را دارا نیستند و نتیجه نهایی بسته 

\subsection*{پ}

صحیح

\subsection*{ت}

نه، این جمله درست نیست. تخفیف در الگوریتم‌های یادگیری تقویتی برای کنترل تأثیر پاداش‌های آینده استفاده می‌شود و به عنوان یک عامل تعدیل‌کننده
\lr{(discounting factor)}
عمل می‌کند. ارزش تخفیف باید بین ۰ و ۱ قرار بگیرد و انتخاب مقدار مناسب آن بسته به مسئله و شرایط خاص است.

تخفیف باعث می‌شود پاداش‌های آینده کمتر ارزش داشته باشند و به ارزش پاداش‌های فعلی بیشتری توجه شود. با تخفیف بالا، عامل به دنبال دریافت پاداش‌های نزدیک در زمان حال است و با تخفیف کم، عامل تمایل دارد به پاداش‌های آینده بیشتر توجه کند. اما استنتاج کلی این نیست که تخفیف کوچکتر از ۱ همواره به عنوان یک پاداش منفی در نظر گرفته می‌شود.

\subsection*{ث}

صحیح