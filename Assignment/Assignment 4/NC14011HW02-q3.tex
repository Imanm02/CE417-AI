\subsection*{الف}
از راست شروع می‌کنیم:
$$p (10 + \gamma \times 0) + (1 - p) (0 + \gamma \times 0) = 10p$$
$$p (0 + \gamma \times 10p) + (1 - p) (0 + \gamma \times 0) = 10p^2 \gamma$$
$$1 (0 + \gamma \times 10p^2 \gamma) = 10p^2 \gamma^2$$
$$1 (0 + \gamma \times 10p^2 \gamma^2) = 10p^2 \gamma^3$$

\subsection*{ب}
از چپ شروع می‌کنیم.
$$1 (5 + \gamma \times 0) = 5$$
$$1 (0 + \gamma \times 5) = 5\gamma$$
$$p (0 + \gamma \times 5\gamma) + (1 - p) (0 + \gamma \times 0) = 5 p \gamma^2$$
$$p (0 + \gamma \times 5p\gamma^2) + (1 - p) (0 + \gamma \times 0) = 5 p^2 \gamma^3$$

\subsection*{ج}
باید معادله‌ی زیر را حل کنیم. به صورت کلی نیز می‌دانیم که
	$0 < p, \gamma < 1$
	است.
	\begin{gather*}
		10p^2 \gamma^2 > 5\gamma \implies 2p^2 \gamma > 1 \implies p^2 > \frac{1}{2\gamma} \implies\\
		p > \frac{1}{\sqrt{2\gamma}}
	\end{gather*}

\subsection*{د}


در یادگیری تقویتی
\lr{(Reinforcement Learning)}
،
 
\lr{Policy Iteration}
و
\lr{Value Iteration}
دو روش اصلی برای یافتن بهترین راه حل
\lr{(optimal policy)}
در یک محیط تصادفی هستند.

\lr{Value Iteration}:
این الگوریتم از تخمین مقدار فعلی یک حالت شروع می کند و سپس برای هر حالت امتیاز تمام اقدامات ممکن را محاسبه می کند. این امتیازات به روز رسانی می شوند و این فرایند تا زمانی که تغییرات کمتر از یک آستانه خاص شود، ادامه می یابد. در نهایت، سیاست بهینه براساس تابع ارزش به روز شده تولید می شود.

\lr{Policy Iteration}:
در این روش، ما از یک سیاست تصادفی شروع می کنیم و سپس در یک حلقه دو قدم را تکرار می‌کنیم:
\lr{(a)}
ثابت کردن سیاست و محاسبه تابع ارزش برای آن سیاست 
\lr{(Policy Evaluation)}
، و
\lr{(b)
بهبود سیاست براساس تابع ارزش 
\lr{(Policy Improvement)}
. این فرایند تا زمانی که سیاست دیگر تغییر نکند، ادامه می‌یابد.

تفاوت اصلی این دو روش در نحوه ی به‌روزرسانی سیاست است. در
\lr{Value Iteration}
، ما فقط یک بار سیاست را به روز می‌کنیم (پس از اینکه تابع ارزش کافیا ثابت شده باشد)، در حالی که در
\lr{Policy Iteration}
، ما سیاست را در هر مرحله به روز می‌کنیم.

یکی از نکاتی که باید توجه داشته باشید این است که در برخی موارد،
\lr{Policy Iteration}
ممکن است سریع‌تر از
\lr{Value Iteration}
به سیاست بهینه برسد، زیرا هر بروزرسانی در 
\lr{Policy Iteration}
می‌تواند به سیاست بهینه نزدی‌ک‌تر شود، در حالی که در
\lr{Value Iteration}
، سیاست بهینه فقط در انتها تولید می‌شود. با این حال،
\lr{Value Iteration}
می‌تواند در محیط هایی با فضای حالت یا عمل بزرگ یا در مواردی که محاسبات
\lr{Policy Evaluation}
زمان‌بر هستند، کارآمدتر باشد.

