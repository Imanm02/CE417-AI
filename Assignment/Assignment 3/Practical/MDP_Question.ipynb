{"cells":[{"cell_type":"markdown","metadata":{"id":"tkuf_W_QI2AK"},"source":["# Markov Decision Process"]},{"cell_type":"markdown","metadata":{"id":"6FN5Q6tOI2AM"},"source":["## Section 1: Introduction\n","\n","In this section, we will introduce the concepts of MDP, Q-values, and V-values. These concepts are fundamental to the field of AI and machine learning, as they are used to model **decision-making problems** in various domains such as \"robotics\", \"finance\", and \"healthcare\".\n","\n","MDP stands for Markov Decision Process. It is a mathematical framework for modeling decision-making problems in which the outcomes are partly random and partly under the control of a decision-maker. MDPs are defined by a set of states, a set of actions, a reward function, and a transition function. The goal is to find a policy that maximizes the expected cumulative reward over time.\n","\n","Q-values and V-values are two important concepts in the context of MDPs. A Q-value represents the expected cumulative reward of taking a particular action in a particular state and following a specific policy thereafter. A V-value represents the expected cumulative reward of being in a particular state and following a specific policy thereafter. These values are used to evaluate and improve the policy of an agent in an MDP."]},{"cell_type":"markdown","metadata":{"id":"kWTbc2DKI2AM"},"source":["## Section 2: The Basics of MDPs\n","\n","In this section, we will explain the basic components of an MDP.\n","\n","An MDP is defined by \"a set of states\", \"a set of actions\", \"a reward function\", and \"a transition function\". The state space is the set of all possible states that the agent can be in. The action space is the set of all possible actions that the agent can take. The reward function defines the reward the agent receives for each action taken in a particular state. The transition function defines the probability of moving from one state to another state after taking a particular action.\n","\n","To illustrate these concepts, let's consider an example of a **robot that needs to navigate through a maze**. The robot can be in one of several states, such as at the start of the maze, at a junction in the maze, or at the end of the maze. This robot takes an action. With Probability of **0.8** It goes in that desired direction but with probability of **0.2** It goes in the perpendicular direction (0.1, 0.1 for each)!\n","\n","In an MDP, the agent interacts with the environment by selecting actions based on its current state and the expected future reward. The goal of the agent is to find a policy that maximizes the expected cumulative reward over time."]},{"cell_type":"markdown","metadata":{"id":"rggAJA_rI2AN"},"source":["**QUESTION**\n","\n","1. What are the state space, action space, reward function, and transition function of the robot in the maze example? Explain why you think each of these components is important for the robot to navigate through the maze.\n","\n","Answer:\n","The state space in the maze example would consist of all possible locations or configurations of the robot within the maze. It includes states such as being at the start of the maze, at various junctions, or at the end of the maze. The state space is important because it provides the robot with the necessary information about its current position, allowing it to make decisions based on its location.\n","\n","The action space represents the available actions that the robot can take at any given state. In the maze example, these actions could include moving in different directions such as up, down, left, or right. The action space is crucial as it provides the robot with the options to navigate through the maze and progress towards the goal.\n","\n","The reward function assigns a numerical reward to the robot based on its actions and states. In the maze example, the reward function would likely provide positive rewards for reaching the goal and negative rewards for colliding with obstacles or deviating from the desired path. The reward function is important as it serves as feedback for the robot's decision-making process, guiding it towards actions that maximize cumulative rewards.\n","\n","The transition function describes the dynamics of the environment and determines the probability of transitioning from one state to another after taking a specific action. In the maze example, the transition function would specify the probability of moving in the desired direction (0.8) or in a perpendicular direction (0.2) after selecting an action. The transition function is essential because it captures the uncertainty in the environment and allows the robot to account for the possible outcomes of its actions.\n","\n","2. Is our environment stochastic or deterministic? Why?!\n","\n","Answer:\n","Based on the information provided, the environment in this maze example is stochastic. This is because there is a probabilistic element involved in the transition from one state to another after taking an action. Specifically, there is a 0.8 probability of moving in the desired direction and a 0.2 probability of moving in a perpendicular direction. The stochastic nature of the environment introduces uncertainty and adds complexity to the robot's decision-making process, requiring it to consider the potential outcomes of its actions and make informed choices."]},{"cell_type":"markdown","metadata":{"id":"v2InylefI2AN"},"source":["**Define The MDP**:\n","\n","Based on your choice of rewards and transitions and the state space, define the MDP for the robot in the maze example. You can complete the following code to define the MDP:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtHucQ2tI2AN","executionInfo":{"status":"ok","timestamp":1688312007969,"user_tz":-210,"elapsed":393,"user":{"displayName":"Iman Mohammadi","userId":"10591440637513622156"}},"outputId":"8ed7476a-5326-4d15-c91e-c39125aefb59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Our Maze is: \n"," [[2 0 0 0 0]\n"," [0 1 1 0 1]\n"," [0 0 0 0 0]\n"," [0 1 1 1 3]]\n"]}],"source":["import numpy as np\n","maze = np.array([[2, 0, 0, 0, 0],\n","                 [0, 1, 1, 0, 1],\n","                 [0, 0, 0, 0, 0],\n","                 [0, 1, 1, 1, 3]])\n","print(\"Our Maze is: \\n\", maze)\n","states = [i for i in range(maze.shape[0]*maze.shape[1])]\n","actions = {0:'up', 1:'down', 2:'left', 3:'right'}\n","rewards = [0 if i!=3 or j!=4 else 10 for i in range(4) for j in range (5) ]\n","transition_probs = [[[0 for i in range (len(actions))] for j in range(len(states))] for k in range (len(states))]\n","for i in range(len(states)):\n","    if (i+1)%5 != 0 and maze[i//5][(i%5)+1]!=1:\n","        transition_probs[i][i+1][3]=0.8\n","        transition_probs[i][i+1][0]=0.1\n","        transition_probs[i][i+1][1]=0.1\n","    else:\n","        transition_probs[i][i][3]=0.8\n","        transition_probs[i][i][0]=0.1\n","        transition_probs[i][i][1]=0.1\n","    if i%5 != 0 and maze[i//5][(i%5)-1]!=1:\n","        transition_probs[i][i-1][2]=0.8\n","        transition_probs[i][i-1][0]=0.1\n","        transition_probs[i][i-1][1]=0.1\n","    else:\n","        transition_probs[i][i][2]=0.8\n","        transition_probs[i][i][0]=0.1\n","        transition_probs[i][i][1]=0.1\n","    if i>4 and maze[(i//5)-1][(i%5)]!=1:\n","        transition_probs[i][i-5][2]=0.1\n","        transition_probs[i][i-5][0]=0.8\n","        transition_probs[i][i-5][3]=0.1\n","    else:\n","        transition_probs[i][i][2]=0.1\n","        transition_probs[i][i][0]=0.8\n","        transition_probs[i][i][3]=0.1\n","    if i<15 and maze[(i//5)+1][(i%5)]!=1:\n","        transition_probs[i][i+5][2]=0.1\n","        transition_probs[i][i+5][1]=0.8\n","        transition_probs[i][i+5][3]=0.1\n","    else:\n","        transition_probs[i][i][2]=0.1\n","        transition_probs[i][i][1]=0.8\n","        transition_probs[i][i][3]=0.1\n","discount = 0.9\n","values = {state: 0 for state in states}\n","q_values = {(state, action): 0 for state in states for action in actions}"]},{"cell_type":"markdown","metadata":{"id":"HXzWGnyTI2AP"},"source":["## Section 3: Computing V-values\n","\n","In this section, we will explain how to compute V-values for an MDP using the Bellman equation.\n","\n","The Bellman equation is a recursive equation that expresses the value of a state in terms of the values of its successor states. It is defined as:\n","\n","$$V(s) = R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n","\n","where V(s) is the value of state s, R(s) is the reward for being in state s, Î³ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, and max_a is the maximum over all possible actions a.\n","\n","To compute the V-values for an MDP, we start with an initial estimate of the V-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n","\n","$$V(s) \\leftarrow R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n","\n","We can use dynamic programming algorithms such as value iteration or policy iteration to compute the V-values.\n","\n","We can use the Bellman equation to compute the V-values for each state in the maze. The V-values represent the expected cumulative reward that the robot can obtain if it starts from that state and follows an optimal policy thereafter. Complete the code below:\n","\n","(**Note:** your final result can be slightly different from the result printed below and it's okay!)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jSfN_9_I2AV","executionInfo":{"status":"ok","timestamp":1688312017842,"user_tz":-210,"elapsed":379,"user":{"displayName":"Iman Mohammadi","userId":"10591440637513622156"}},"outputId":"634c0a06-f978-4067-bd33-c792a509a3c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["V-values:\n","10.901132229407896 12.24695103126669 15.47878533827593 19.563464808748375 15.478785342743663 \n","12.246951031266693 15.798417302076189 20.74114711905716 23.30178257078303 29.450864086830038 \n","15.478785338275934 18.436575216939698 23.30178257078303 29.450864086830038 34.30989706955881 \n","12.246951036372677 15.798417306656981 18.436575220488354 34.30989706955881 28.571428565746047 \n"]}],"source":["def value_iteration(states, actions, rewards, transition_probs, discount, values, theta=1e-8):\n","    while True:\n","        delta = 0\n","        for state in states:\n","            old_value = values[state]\n","            new_values = []\n","            for action in actions:\n","                action_value = 0\n","                for next_state in states:\n","                    action_value += transition_probs[state][next_state][action] * (rewards[next_state] + discount * values[next_state])\n","                new_values.append(action_value)\n","            values[state] = max(new_values)\n","            delta = max(delta, abs(old_value - values[state]))\n","        if delta < theta:\n","            break\n","    return values\n","\n","values = value_iteration(states, actions, rewards, transition_probs, discount, values)\n","\n","print(\"V-values:\")\n","for i in range(maze.shape[0]):\n","    for j in range(maze.shape[1]):\n","        print(values[i*maze.shape[1]+j], end=' ')\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"K3jleZt3I2AV"},"source":["## Section 4: Computing Q-values\n","\n","In this section, we will explain how to compute Q-values for an MDP using the Bellman equation.\n","\n","The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. The Q-values can be computed using the Bellman equation as follows:\n","\n","$$Q(s, a) = R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n","\n","where Q(s, a) is the Q-value of state-action pair (s, a), R(s, a) is the reward for taking action a in state s, Î³ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, max_a' is the maximum over all possible actions a' in state s', and sum_s' is the sum over all possible successor states s' of state s.\n","\n","To compute the Q-values for an MDP, we start with an initial estimate of the Q-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n","\n","$$Q(s, a) \\leftarrow R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n","\n","We can use dynamic programming algorithms such as Q-learning or SARSA to compute the Q-values.\n","\n","\n","We can use the Q-learning algorithm to compute the Q-values for each state-action pair in the maze. The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. Complete the code below:\n","\n","(**Note:** your final result can be slightly different from the result printed below and it's okay!)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhsewbPhI2AW","executionInfo":{"status":"ok","timestamp":1688312110247,"user_tz":-210,"elapsed":469,"user":{"displayName":"Iman Mohammadi","userId":"10591440637513622156"}},"outputId":"0439733d-6491-4b28-fbfd-e56ee5bc72d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Q-values:\n","Q(0, up): 8.951040797699095\n","Q(0, down): 10.901132235216561\n","Q(0, left): 2.0833274939023556\n","Q(0, right): 10.901132235765273\n","Q(1, up): 11.191997323455631\n","Q(1, down): 11.191997323455631\n","Q(1, left): 8.951040802483861\n","Q(1, right): 12.246951035723574\n","Q(2, up): 14.007662868931066\n","Q(2, down): 14.007662868931066\n","Q(2, left): 10.210895426094813\n","Q(2, right): 15.478785342175703\n","Q(3, up): 16.87187602339349\n","Q(3, down): 19.563464812160674\n","Q(3, left): 15.0025977107821\n","Q(3, right): 15.0025977107821\n","Q(4, up): 12.905437279460969\n","Q(4, down): 12.905437279460969\n","Q(4, left): 15.4787853455515\n","Q(4, right): 1.3930906810996353\n","Q(5, up): 8.951040802483861\n","Q(5, down): 12.246951035723573\n","Q(5, left): 11.191997327313686\n","Q(5, right): 11.191997327313686\n","Q(6, up): 11.341887896050164\n","Q(6, down): 15.798417306074645\n","Q(6, left): 11.579322108404279\n","Q(6, right): 14.136377823057051\n","Q(7, up): 15.108589118339344\n","Q(7, down): 20.741147122541882\n","Q(7, left): 18.423877040347104\n","Q(7, right): 20.26753456268599\n","Q(8, up): 16.182855096076818\n","Q(8, down): 23.301782573493373\n","Q(8, left): 21.18857305378122\n","Q(8, right): 21.18857305378122\n","Q(9, up): 15.892463648183016\n","Q(9, down): 29.450864089201588\n","Q(9, left): 21.25826487024182\n","Q(9, right): 25.685603561551737\n","Q(10, up): 11.870187195562995\n","Q(10, down): 11.870187195562995\n","Q(10, left): 13.349176629420953\n","Q(10, right): 15.478785342175703\n","Q(11, up): 16.764585267862405\n","Q(11, down): 16.764585267862405\n","Q(11, left): 12.80401721583469\n","Q(11, right): 18.43657522003723\n","Q(12, up): 21.087152988143927\n","Q(12, down): 21.087152988143927\n","Q(12, left): 15.371494589747936\n","Q(12, right): 23.301782573493373\n","Q(13, up): 21.96233462075659\n","Q(13, down): 26.389673310013603\n","Q(13, left): 21.525021652301163\n","Q(13, right): 29.450864089201588\n","Q(14, up): 27.353703657843795\n","Q(14, down): 34.30989707138998\n","Q(14, left): 27.863941451542104\n","Q(14, right): 6.659319307316959\n","Q(15, up): 12.24695103958163\n","Q(15, down): 8.817804748498773\n","Q(15, left): 2.49531627435816\n","Q(15, right): 2.49531627435816\n","Q(16, up): 15.798417309535871\n","Q(16, down): 12.477086056428174\n","Q(16, left): 11.898954076160352\n","Q(16, right): 3.081149327661579\n","Q(17, up): 18.436575222718577\n","Q(17, down): 13.274334160357377\n","Q(17, left): 3.7564522016590756\n","Q(17, right): 3.7564522016590756\n","Q(18, up): 27.863941451343962\n","Q(18, down): 28.274554460707513\n","Q(18, left): 5.738468504255099\n","Q(18, right): 34.30989707138998\n","Q(19, up): 28.27455446229265\n","Q(19, down): 28.571428567134884\n","Q(19, left): 6.659319307467237\n","Q(19, right): 6.659319307467237\n"]}],"source":["def q_learning(states, actions, rewards, transition_probs, discount, q_values, theta=1e-8):\n","    while True:\n","        delta = 0\n","        for state in states:\n","            for action in actions:\n","                old_q_value = q_values[(state, action)]\n","                new_q_values = []\n","                for next_state in states:\n","                    next_q_values = []\n","                    for next_action in actions:\n","                        next_q_values.append(transition_probs[state][next_state][action] * (rewards[next_state] + discount * q_values[(next_state, next_action)]))\n","                    new_q_values.append(max(next_q_values))\n","                q_values[(state, action)] = sum(new_q_values)\n","                delta = max(delta, abs(old_q_value - q_values[(state, action)]))\n","        if delta < theta:\n","            break\n","    return q_values\n","\n","q_values = q_learning(states, actions, rewards, transition_probs, discount, q_values)\n","\n","print(\"Q-values:\")\n","for state in states:\n","    for action in actions:\n","        print(f\"Q({state}, {actions[action]}): {q_values[(state, action)]}\")"]},{"cell_type":"markdown","metadata":{"id":"6HMdeBW3I2AW"},"source":["## Section 5: Visualizing the Optimal Policy\n","\n","Now that we have computed the Q-values, we can use them to find the optimal policy, which is the sequence of actions that the robot should take in each state to maximize its expected reward. We can visualize the optimal policy as arrows in a grid, where each arrow corresponds to the action with the highest Q-value in the corresponding state. Complete the code below:\n","\n","(**Note:** your final result can be slightly different from the result printed below and it's okay!)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F021T_k7I2AW","executionInfo":{"status":"ok","timestamp":1688312160439,"user_tz":-210,"elapsed":3,"user":{"displayName":"Iman Mohammadi","userId":"10591440637513622156"}},"outputId":"26ba24a4-aa9f-4fd0-e10c-f9a9b7fa5f6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal policy:\n","right right right down left \n","down down down down down \n","right right right right down \n","up up up right down \n","{(0, 0): 8.951040797699095, (0, 1): 10.901132235216561, (0, 2): 2.0833274939023556, (0, 3): 10.901132235765273, (1, 0): 11.191997323455631, (1, 1): 11.191997323455631, (1, 2): 8.951040802483861, (1, 3): 12.246951035723574, (2, 0): 14.007662868931066, (2, 1): 14.007662868931066, (2, 2): 10.210895426094813, (2, 3): 15.478785342175703, (3, 0): 16.87187602339349, (3, 1): 19.563464812160674, (3, 2): 15.0025977107821, (3, 3): 15.0025977107821, (4, 0): 12.905437279460969, (4, 1): 12.905437279460969, (4, 2): 15.4787853455515, (4, 3): 1.3930906810996353, (5, 0): 8.951040802483861, (5, 1): 12.246951035723573, (5, 2): 11.191997327313686, (5, 3): 11.191997327313686, (6, 0): 11.341887896050164, (6, 1): 15.798417306074645, (6, 2): 11.579322108404279, (6, 3): 14.136377823057051, (7, 0): 15.108589118339344, (7, 1): 20.741147122541882, (7, 2): 18.423877040347104, (7, 3): 20.26753456268599, (8, 0): 16.182855096076818, (8, 1): 23.301782573493373, (8, 2): 21.18857305378122, (8, 3): 21.18857305378122, (9, 0): 15.892463648183016, (9, 1): 29.450864089201588, (9, 2): 21.25826487024182, (9, 3): 25.685603561551737, (10, 0): 11.870187195562995, (10, 1): 11.870187195562995, (10, 2): 13.349176629420953, (10, 3): 15.478785342175703, (11, 0): 16.764585267862405, (11, 1): 16.764585267862405, (11, 2): 12.80401721583469, (11, 3): 18.43657522003723, (12, 0): 21.087152988143927, (12, 1): 21.087152988143927, (12, 2): 15.371494589747936, (12, 3): 23.301782573493373, (13, 0): 21.96233462075659, (13, 1): 26.389673310013603, (13, 2): 21.525021652301163, (13, 3): 29.450864089201588, (14, 0): 27.353703657843795, (14, 1): 34.30989707138998, (14, 2): 27.863941451542104, (14, 3): 6.659319307316959, (15, 0): 12.24695103958163, (15, 1): 8.817804748498773, (15, 2): 2.49531627435816, (15, 3): 2.49531627435816, (16, 0): 15.798417309535871, (16, 1): 12.477086056428174, (16, 2): 11.898954076160352, (16, 3): 3.081149327661579, (17, 0): 18.436575222718577, (17, 1): 13.274334160357377, (17, 2): 3.7564522016590756, (17, 3): 3.7564522016590756, (18, 0): 27.863941451343962, (18, 1): 28.274554460707513, (18, 2): 5.738468504255099, (18, 3): 34.30989707138998, (19, 0): 28.27455446229265, (19, 1): 28.571428567134884, (19, 2): 6.659319307467237, (19, 3): 6.659319307467237}\n"]}],"source":["def find_optimal_policy(states, actions, q_values):\n","    optimal_policy = {state: None for state in states}\n","    for state in states:\n","        optimal_action = max(actions, key=lambda action: q_values[(state, action)])\n","        optimal_policy[state] = actions[optimal_action]\n","    return optimal_policy\n","\n","optimal_policy = find_optimal_policy(states, actions, q_values)\n","\n","print(\"Optimal policy:\")\n","for i in range(maze.shape[0]):\n","    for j in range(maze.shape[1]):\n","        print(optimal_policy[i*maze.shape[1]+j], end=' ')\n","    print()\n","\n","print(q_values)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwFxgBvvI2AX","executionInfo":{"status":"ok","timestamp":1688312212415,"user_tz":-210,"elapsed":3,"user":{"displayName":"Iman Mohammadi","userId":"10591440637513622156"}},"outputId":"d9b24bc6-b3d4-4f6a-c612-481ed9d969a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["S â â â â\n","â X X â X\n","â â â â â\n","â X X X G\n"]}],"source":["actions_signs = {'up': 'â', 'down': 'â', 'left': 'â', 'right': 'â'}\n","\n","policy_grid = [[' ' for _ in range(maze.shape[1])] for _ in range(maze.shape[0])]\n","\n","for i in range(maze.shape[0]):\n","    for j in range(maze.shape[1]):\n","        state = i * maze.shape[1] + j\n","        policy_grid[i][j] = actions_signs[optimal_policy[state]]\n","\n","for i in range(maze.shape[0]):\n","    for j in range(maze.shape[1]):\n","        if maze[i][j] == 2:\n","            policy_grid[i][j] = 'S'\n","        elif maze[i][j] == 3:\n","            policy_grid[i][j] = 'G'\n","        elif maze[i][j] == 1:\n","            policy_grid[i][j] = 'X'\n","\n","for row in policy_grid:\n","    print(' '.join(row))"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}