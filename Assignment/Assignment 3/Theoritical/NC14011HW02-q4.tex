\subsection*{الف}

برای اینکه تخمین‌گر
MLE
(تخمین‌گر بیشینه درستنمایی) برای مقدار
\(\theta\)
را بیابیم، می‌خواهیم درستنمایی داده‌ها را بیشینه کنیم. 

فرض کنیم که
\(Y_1, Y_2, \ldots, Y_n\)
نمونه‌هایی از توزیع
\(\theta + x\)
باشند که دریافت کرده‌ایم. چون x نویز گاوسی با میانگین صفر و واریانس 1 است، پس
\(Y = \theta + x\)
نیز یک توزیع گاوسی با میانگین
\(\theta\)
و واریانس 1 دارد.

درست‌نمایی (likelihood) این داده‌ها با توجه به مقدار \(\theta\) به صورت زیر است:

\[
L(\theta; Y_1, Y_2, \ldots, Y_n) = f(Y_1, Y_2, \ldots, Y_n; \theta) 
\]

\[
= f(Y_1; \theta) \cdot f(Y_2; \theta) \cdot \ldots \cdot f(Y_n; \theta)
\]

\[
= \frac{1}{\sqrt{2 \pi}} e^{-(Y_1 - \theta)^2 / 2} \cdot \frac{1}{\sqrt{2 \pi}} e^{-(Y_2 - \theta)^2 / 2} \cdot \ldots \cdot \frac{1}{\sqrt{2 \pi}} e^{-(Y_n - \theta)^2 / 2}
\]

\[
= \frac{1}{ (2 \pi)^{n/2}} e^{-\frac{1}{2} \sum_{i=1}^{n} (Y_i - \theta)^2}
\]

برای بیشینه کردن این تابع درستنمایی، می‌توانیم به جای آن لگاریتم درستنمایی را بیشینه کنیم:

\[
l(\theta) = \log(L(\theta; Y_1, Y_2, \ldots, Y_n))
\]

\[
= -\frac{n}{2} \log(2 \pi) - \frac{1}{2} \sum_{i=1}^{n} (Y_i - \theta)^2
\]

برای بیشینه‌سازی \(l(\theta)\) نیاز است که مشتق آن نسبت به \(\theta\) را برابر صفر قرار دهیم:

\[
\frac{dl(\theta)}{d\theta} = \sum_{i=1}^{n} (Y_i - \theta) = 0
\]

از این معادله به دست می‌آید که:

\[
\hat{\theta}_{MLE} = \frac{\sum_{i=1}^{n} Y_i}{n}
\]

پس تخمین‌گر MLE برای \(\theta\) میانگین نمونه‌ها است.

\subsection*{ب}

حالا برای بررسی consistent بودن (یا همگرایی) تخمین‌گر MLE برای \(\theta\)، باید نشان دهیم که هنگامی که تعداد نمونه‌ها (n) به سمت بی‌نهایت میل می‌کند، تخمین‌گر MLE به مقدار واقعی \(\theta\) همگرا می‌شود. به عبارت دیگر:

با فرض \(\hat{\theta}_{MLE,n}\) به عنوان تخمین‌گر MLE برای \(\theta\) بر اساس n نمونه، می‌خواهیم نشان دهیم که \(\hat{\theta}_{MLE,n}\) به \(\theta\) همگرا می‌شود هنگامی که \(n \rightarrow \infty\). 

از قانون اعداد بزرگ می‌دانیم که اگر \(Y_1, Y_2, \ldots, Y_n\) متغیرهای تصادفی
 
\lr{i.i.d (independent and identically distributed)}
 باشند و
\(\mathbb{E}[Y_i] = \mu\)
 ، آنگاه میانگین نمونه 
\(\bar{Y}_n = \frac{1}{n}\sum_{i=1}^{n} Y_i\) به \(\mu\)
همگرا می‌شود هنگامی که
\(n \rightarrow \infty\). 

در این مورد،
\(Y_i = \theta + x\) و \(x \sim N(0,1)\)
، بنابراین
\(\mathbb{E}[Y_i] = \mathbb{E}[\theta + x] = \theta + \mathbb{E}[x] = \theta\)
زیرا
\(\mathbb{E}[x] = 0\)
است.

پس بر اساس قانون اعداد بزرگ،
\(\hat{\theta}_{MLE,n} = \frac{1}{n}\sum_{i=1}^{n} Y_i\)
به
\(\theta\)
همگرا می‌شود هنگامی که \(n \rightarrow \infty\).

بنابراین، تخمین‌گر MLE برای \(\theta\) در این مورد consistent است.

\subsection*{ج}

ما با یک مسئله‌ی استنتاج بیزی روبرو هستیم، که در آن مقدار \(\theta\) یکی از دو مقدار ممکن 1 یا -1 است و توزیع prior آن یک توزیع برنولی با پارامتر \(p\) است. 

یعنی 

\[
P(\theta = 1) = p
\]

و 

\[
P(\theta = -1) = 1 - p
\]

با فرض \(Y\) به عنوان مشاهده شده، می‌خواهیم تخمین‌گر بیزی \(\theta\) را با استفاده از قاعده بیز پیدا کنیم. بر اساس قاعده بیز، 

\[
P(\theta | Y) = \frac{P(Y | \theta) P(\theta)}{P(Y)}
\]

از آنجا که \(P(Y)\) برای تمام \(\theta\)‌ها یکسان است، می‌توانیم از آن صرف‌نظر کنیم و فقط به صورت نسبی توزیع‌ها را بررسی کنیم. این به ما اجازه می‌دهد تا \(\theta\) را انتخاب کنیم به طوری که \(P(Y | \theta) P(\theta)\) را بیشینه کند.

برای \(\theta = 1\)، 

\[
P(Y | \theta = 1) P(\theta = 1) = \frac{1}{\sqrt{2 \pi}} e^{-(Y - 1)^2 / 2} p
\]

و برای \(\theta = -1\)، 

\[
P(Y | \theta = -1) P(\theta = -1) = \frac{1}{\sqrt{2 \pi}} e^{-(Y + 1)^2 / 2} (1 - p)
\]

حالا تخمین‌گر بیزی \(\theta\) را می‌توانیم به این شکل بیان کنیم:

\[
\hat{\theta}_{Bayes} = 
\begin{cases} 
	1 & \text{if } \frac{1}{\sqrt{2 \pi}} e^{-(Y - 1)^2 / 2} p > \frac{1}{\sqrt{2 \pi}} e^{-(Y + 1)^2 / 2} (1 - p) \\
	-1 & \text{otherwise}
\end{cases}
\]


\subsection*{د}

با استفاده از تخمین‌گر LMMSE داریم:

$$
\alpha_i = \frac{Cov (\theta , y_i)}{Var (Y_i)}
$$